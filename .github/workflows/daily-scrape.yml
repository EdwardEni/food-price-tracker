name: Daily Scraper Run

on:
  schedule:
    # Runs every day at 2:00 AM UTC, adjust as needed
    - cron: '0 2 * * *'
  workflow_dispatch:

jobs:
  run-scraper-and-loader:
    runs-on: ubuntu-latest

    services:
      postgres:
        image: postgres:latest
        env:
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: discombobulated
          POSTGRES_DB: food_price_db
        ports:
          - 5432:5432
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    steps:
      - name: Checkout repo
        uses: actions/checkout@v3

      - name: List files for debug 
        run: ls -R

      - name: Run ETL script via module import
        run: python -c "import src.etl.load_to_db"

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'      # Use a stable Python version for GitHub Actions

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Run scraper script (download CSV)
        run: python src/scraper/scrape.py

      - name: Check load_to_db.py existence
        run: |
          if [ -f src/etl/load_to_db.py ]; then
            echo "File exists"
          else
            echo "File NOT found"
            ls -l src/etl
            exit 1
          fi

      # Ensure your load_to_db.py uses this database URL:
      # postgresql://postgres:discombobulated@localhost:5432/food_price_db

      - name: Run data loader script (load to DB)
        run: python src/etl/load_to_db.py

      - name: Upload downloaded raw CSV
        uses: actions/upload-artifact@v4
        with:
          name: raw-wfp-food-prices-csv
          path: data/raw/wfpvam_foodprices.csv
